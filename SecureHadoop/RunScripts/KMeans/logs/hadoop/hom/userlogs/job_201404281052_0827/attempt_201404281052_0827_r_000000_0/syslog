2014-05-02 20:08:24,059 INFO org.apache.hadoop.util.NativeCodeLoader: Loaded the native-hadoop library
2014-05-02 20:08:24,263 INFO org.apache.hadoop.mapred.TaskRunner: Creating symlink: /home/dinhtta/Hadoop/tmp/mapred/local/taskTracker/dinhtta/distcache/3004027326935359461/localhost/home/dinhtta/Hadoop/tmp/mapred/staging/dinhtta/.staging/job_201404281052_0827/files/libhryto.so <- /home/dinhtta/Hadoop/tmp/mapred/local/taskTracker/dinhtta/jobcache/job_201404281052_0827/attempt_201404281052_0827_r_000000_0/work/./libhryto.so
2014-05-02 20:08:24,274 INFO org.apache.hadoop.mapreduce.filecache.TrackerDistributedCacheManager: Creating symlink: /home/dinhtta/Hadoop/tmp/mapred/local/taskTracker/dinhtta/jobcache/job_201404281052_0827/jars/.job.jar.crc <- /home/dinhtta/Hadoop/tmp/mapred/local/taskTracker/dinhtta/jobcache/job_201404281052_0827/attempt_201404281052_0827_r_000000_0/work/./.job.jar.crc
2014-05-02 20:08:24,275 INFO org.apache.hadoop.mapreduce.filecache.TrackerDistributedCacheManager: Creating symlink: /home/dinhtta/Hadoop/tmp/mapred/local/taskTracker/dinhtta/jobcache/job_201404281052_0827/jars/job.jar <- /home/dinhtta/Hadoop/tmp/mapred/local/taskTracker/dinhtta/jobcache/job_201404281052_0827/attempt_201404281052_0827_r_000000_0/work/./job.jar
2014-05-02 20:08:24,279 INFO org.apache.hadoop.metrics.jvm.JvmMetrics: Initializing JVM Metrics with processName=SHUFFLE, sessionId=
2014-05-02 20:08:24,279 WARN org.apache.hadoop.conf.Configuration: user.name is deprecated. Instead, use mapreduce.job.user.name
2014-05-02 20:08:24,402 INFO org.apache.hadoop.mapreduce.util.ProcessTree: setsid exited with exit code 0
2014-05-02 20:08:24,409 INFO org.apache.hadoop.mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.mapreduce.util.LinuxResourceCalculatorPlugin@48688905
2014-05-02 20:08:24,499 WARN org.apache.hadoop.conf.Configuration: session.id is deprecated. Instead, use dfs.metrics.session-id
2014-05-02 20:08:24,522 INFO org.apache.hadoop.mapreduce.task.reduce.MergeManager: MergerManager: memoryLimit=623765056, maxSingleShuffleLimit=155941264, mergeThreshold=411684960, ioSortFactor=100, memToMemMergeOutputsThreshold=100
2014-05-02 20:08:24,528 INFO org.apache.hadoop.mapreduce.task.reduce.EventFetcher: attempt_201404281052_0827_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
2014-05-02 20:08:24,537 INFO org.apache.hadoop.mapreduce.task.reduce.EventFetcher: attempt_201404281052_0827_r_000000_0: Got 2 new map-outputs
2014-05-02 20:08:24,538 INFO org.apache.hadoop.mapreduce.task.reduce.ShuffleScheduler: Assiging homer:50060 with 2 to fetcher#1
2014-05-02 20:08:24,538 INFO org.apache.hadoop.mapreduce.task.reduce.ShuffleScheduler: assigned 2 of 2 to homer:50060 to fetcher#1
2014-05-02 20:08:25,024 INFO org.apache.hadoop.mapreduce.task.reduce.Fetcher: for url=50060/mapOutput?job=job_201404281052_0827&reduce=0&map=attempt_201404281052_0827_m_000000_0,attempt_201404281052_0827_m_000001_0 sent hash and receievd reply
2014-05-02 20:08:25,026 INFO org.apache.hadoop.mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_201404281052_0827_m_000000_0 decomp: 19062 len: 19066 to MEMORY
2014-05-02 20:08:25,030 INFO org.apache.hadoop.mapreduce.task.reduce.Fetcher: Read 19062 bytes from map-output for attempt_201404281052_0827_m_000000_0
2014-05-02 20:08:25,030 INFO org.apache.hadoop.mapreduce.task.reduce.MergeManager: closeInMemoryFile -> map-output of size: 19062, inMemoryMapOutputs.size() -> 1
2014-05-02 20:08:25,032 INFO org.apache.hadoop.mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_201404281052_0827_m_000001_0 decomp: 19062 len: 19066 to MEMORY
2014-05-02 20:08:25,034 INFO org.apache.hadoop.mapreduce.task.reduce.Fetcher: Read 19062 bytes from map-output for attempt_201404281052_0827_m_000001_0
2014-05-02 20:08:25,034 INFO org.apache.hadoop.mapreduce.task.reduce.MergeManager: closeInMemoryFile -> map-output of size: 19062, inMemoryMapOutputs.size() -> 2
2014-05-02 20:08:25,036 INFO org.apache.hadoop.mapreduce.task.reduce.ShuffleScheduler: homer:50060 freed by fetcher#1 in 498s
2014-05-02 20:08:42,559 INFO org.apache.hadoop.mapreduce.task.reduce.ShuffleScheduler: Assiging homer:50060 with 1 to fetcher#1
2014-05-02 20:08:42,560 INFO org.apache.hadoop.mapreduce.task.reduce.ShuffleScheduler: assigned 1 of 1 to homer:50060 to fetcher#1
2014-05-02 20:08:42,560 INFO org.apache.hadoop.mapreduce.task.reduce.EventFetcher: attempt_201404281052_0827_r_000000_0: Got 2 new map-outputs
2014-05-02 20:08:42,564 INFO org.apache.hadoop.mapreduce.task.reduce.Fetcher: for url=50060/mapOutput?job=job_201404281052_0827&reduce=0&map=attempt_201404281052_0827_m_000002_0 sent hash and receievd reply
2014-05-02 20:08:42,564 INFO org.apache.hadoop.mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_201404281052_0827_m_000002_0 decomp: 19062 len: 19066 to MEMORY
2014-05-02 20:08:42,565 INFO org.apache.hadoop.mapreduce.task.reduce.Fetcher: Read 19062 bytes from map-output for attempt_201404281052_0827_m_000002_0
2014-05-02 20:08:42,566 INFO org.apache.hadoop.mapreduce.task.reduce.MergeManager: closeInMemoryFile -> map-output of size: 19062, inMemoryMapOutputs.size() -> 3
2014-05-02 20:08:42,566 INFO org.apache.hadoop.mapreduce.task.reduce.ShuffleScheduler: homer:50060 freed by fetcher#1 in 6s
2014-05-02 20:08:42,566 INFO org.apache.hadoop.mapreduce.task.reduce.ShuffleScheduler: Assiging homer:50060 with 1 to fetcher#1
2014-05-02 20:08:42,566 INFO org.apache.hadoop.mapreduce.task.reduce.ShuffleScheduler: assigned 1 of 1 to homer:50060 to fetcher#1
2014-05-02 20:08:42,569 INFO org.apache.hadoop.mapreduce.task.reduce.Fetcher: for url=50060/mapOutput?job=job_201404281052_0827&reduce=0&map=attempt_201404281052_0827_m_000003_0 sent hash and receievd reply
2014-05-02 20:08:42,569 INFO org.apache.hadoop.mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_201404281052_0827_m_000003_0 decomp: 19062 len: 19066 to MEMORY
2014-05-02 20:08:42,570 INFO org.apache.hadoop.mapreduce.task.reduce.Fetcher: Read 19062 bytes from map-output for attempt_201404281052_0827_m_000003_0
2014-05-02 20:08:42,570 INFO org.apache.hadoop.mapreduce.task.reduce.MergeManager: closeInMemoryFile -> map-output of size: 19062, inMemoryMapOutputs.size() -> 4
2014-05-02 20:08:42,571 INFO org.apache.hadoop.mapreduce.task.reduce.ShuffleScheduler: homer:50060 freed by fetcher#1 in 5s
2014-05-02 20:08:42,573 INFO org.apache.hadoop.mapreduce.task.reduce.MergeManager: finalMerge called with 4 in-memory map-outputs and 0 on-disk map-outputs
2014-05-02 20:08:42,626 INFO org.apache.hadoop.mapred.Merger: Merging 4 sorted segments
2014-05-02 20:08:42,626 INFO org.apache.hadoop.mapred.Merger: Down to the last merge-pass, with 4 segments left of total size: 76216 bytes
2014-05-02 20:08:42,636 INFO org.apache.hadoop.mapreduce.task.reduce.MergeManager: Merged 4 segments, 76248 bytes to disk to satisfy reduce memory limit
2014-05-02 20:08:42,637 INFO org.apache.hadoop.mapreduce.task.reduce.MergeManager: Merging 1 files, 76246 bytes from disk
2014-05-02 20:08:42,651 INFO org.apache.hadoop.mapreduce.task.reduce.MergeManager: Merging 0 segments, 0 bytes from memory into reduce
2014-05-02 20:08:42,651 INFO org.apache.hadoop.mapred.Merger: Merging 1 sorted segments
2014-05-02 20:08:42,668 INFO org.apache.hadoop.mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 76234 bytes
2014-05-02 20:08:42,834 WARN org.apache.hadoop.conf.Configuration: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
2014-05-02 20:08:43,530 INFO org.apache.hadoop.mapred.Task: Task:attempt_201404281052_0827_r_000000_0 is done. And is in the process of commiting
2014-05-02 20:08:44,571 INFO org.apache.hadoop.mapred.Task: Task attempt_201404281052_0827_r_000000_0 is allowed to commit now
2014-05-02 20:08:44,620 INFO org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: Saved output of task 'attempt_201404281052_0827_r_000000_0' to /HiBench/KMeans/EncryptedOutput/clusters-1
2014-05-02 20:08:44,651 INFO org.apache.hadoop.mapred.Task: Task 'attempt_201404281052_0827_r_000000_0' done.
